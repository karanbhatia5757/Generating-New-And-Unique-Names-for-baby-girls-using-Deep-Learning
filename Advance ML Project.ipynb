{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Machine Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open function opens a file for reading. 'r' meaning open file in a reading mode.\n",
    "# open function returns a file object.\n",
    "# read function pulls the entire content of the file as a string and saves it in a variable named names.\n",
    "\n",
    "names = open('babygirls.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lower function makes all names as lower case. \n",
    "# We do this because we want to use 26 alphabets instead of 52(treating lower and upper case as same.)\n",
    "names = names.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names is a long string having all names in a single string separated by \\n\n",
    "# To extract unique letters, we use the set function of python.\n",
    "# list converts the set into a list. It becomes easy to index at a later stage.\n",
    "characters = list(set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_characters = len(names)\n",
    "unique_characters = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7756 total characters and 27 unique characters in the dataset we use.\n"
     ]
    }
   ],
   "source": [
    "print('There are %d total characters and %d unique characters in the dataset we use.' % (total_characters, unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Converting characters to indexes between 0-26 including \\n. This \\n acts as end of a name.\n",
    "# We will use python dictionary i.e. a hash table.\n",
    "# Enumerate function accesses elements from a list i.e. characters here alomg with the indexes.\n",
    "# sorted functed sorts the list.\n",
    "character_to_indexes = {ch:i for i,ch in enumerate(sorted(characters)) }\n",
    "index_to_characters = {i:ch for i,ch in enumerate(sorted(characters))}\n",
    "print(character_to_indexes)\n",
    "print(index_to_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Exploding Gradients.\n",
    "One of the problems in Recurrent Neural Networks is that the gradients can become too large(Exploding Gradients). This problem is solved using gradient clipping. We restrict the range of values that gradients can take. This is done using the clip function of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_clip(gradients, max_value):\n",
    "    \"\"\"Function Parameters: gradients: Python dictionary containing all gradient vectors/matrices.\n",
    "    max_value: The range of value to which we want to restrict the gradients.\"\"\"\n",
    "    \n",
    "    # Accessing the gradients from the dictionary.\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "    \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:      # Looping over the list of gradient vectors.\n",
    "        # Clip function clips the values of gradient vector b/w -max_value to max_value element wise.\n",
    "        # If an element of vector is greater than max_value, it will become equal to max_value and same for less than \n",
    "        # max_value.\n",
    "        # If value lies between -max_value to max_value, it remains the same.\n",
    "        np.clip(gradient,-max_value,max_value,out=gradient) \n",
    "        \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax Function\n",
    "def softmax(x):\n",
    "    # x-np.max(x) is done because np.exp(large number) can become infinity.\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Characters from a trained Network.\n",
    "Assuming that we have an already trained network, we write a function to sample characters from the trained RNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(parameters , character_to_indexes):\n",
    "    \"\"\"Function Parameters: parameters: a dictionary containing model parmeters\n",
    "        character_to_indexes: dictionary mapping characters to their indexes.\n",
    "        \n",
    "        Return: Function returns a list containing indices generated.\"\"\"\n",
    "    \n",
    "    # Retrieving parameters from the dictionary. \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    vocab_size = by.shape[0]   # Size of Output vector y which is same at all time steps.\n",
    "    \n",
    "    n_a = Waa.shape[0]         # Size of activation vector.\n",
    "    \n",
    "    # Initializing vector x for first character input. It will be zero.\n",
    "    # We could create a vector of random values as well.\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Initializing a_prev as zeros.\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Initializing an empty list of indices, which will contain list of indices from which we generate names.\n",
    "    indices = []\n",
    "    \n",
    "    # Flag to detect a newline character.\n",
    "    next_character = -1\n",
    "    \n",
    "    # Looping over time steps 't' to forward propagate and predict output y i.e. index of character generated.\n",
    "    # We sample from a probability distribution randomly at each time step.\n",
    "    # Also, we don't want to have names larger than say(20) characters. So, we use counter to stop the loop.\n",
    "    # Counter will also prevent from entering into an infinite loop.\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    newline_character = character_to_indexes['\\n']\n",
    "    \n",
    "    # Looping over the time steps 't' \n",
    "    while( next_character != newline_character and counter != 20):\n",
    "       \n",
    "        # Forward Propagating 'x'\n",
    "        # Calculating the activation.\n",
    "        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        # Calculating the output vector.\n",
    "        z = np.dot(Wya,a) + by\n",
    "        # Taking softmax of the output vector.\n",
    "        y = softmax(z)\n",
    "        \n",
    "        \n",
    "        # np.random.choice chooses an index from [0,1,...,26] using probability distribution given by y.\n",
    "        # y is vector having probability of choosing an index.\n",
    "        # We could also choose argmax of y but we want to sample randomly.\n",
    "        next_character = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "        \n",
    "        # Appending the index to the indices list.\n",
    "        indices.append(next_character)\n",
    "        \n",
    "        # Overwriting the input character as the one corresponding to the sampled index.\n",
    "        # To be input for next timer step 't'.\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[next_character] = 1\n",
    "        \n",
    "        # Updating a_prev to be 'a' for next time step.\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "        # Increasing the counter.\n",
    "        counter += 1\n",
    "        \n",
    "    \n",
    "    if (counter == 50):\n",
    "        indices.append(character_to_indexes['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 23, 24, 14, 7, 2, 10, 23, 25, 12, 3, 24, 15, 24, 3, 20, 3, 17, 4, 0]\n",
      "list of sampled characters: ['l', 'w', 'x', 'n', 'g', 'b', 'j', 'w', 'y', 'l', 'c', 'x', 'o', 'x', 'c', 't', 'c', 'q', 'd', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "vocab_size = len(characters)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, character_to_indexes)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [index_to_characters[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Language Model Using RNN:\n",
    "It is a 4 Steps Process:<br>\n",
    "1) Forward Propagate <br>\n",
    "2) Calculate Loss<br>\n",
    "3) Find Gradients<br>\n",
    "4) Update Parameters<br>\n",
    "\n",
    "We are using Stochastic Gradient Descent i.e. using 1 example at a time to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Function runs 1 time step forward propagation through a RNN Cell.\n",
    "def rnn_cell_forward(parameters, a_prev, x):\n",
    "    \n",
    "    \"\"\"Function Parameters: parameters: A python dictionary containing network parameters\n",
    "        a_prev: activation of previous time step\n",
    "        x: Input of current time step.\"\"\"\n",
    "    # Retrieving Python Dictionary.\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    # Computing Hidden State Activation.\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) \n",
    "    # Current time step output.\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)  \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x , a , a_prev):\n",
    "    \"\"\"Function Parameters: dy: Derivative of cost w.r.t output y\n",
    "       Gradients: Python dictionary containing gradients\n",
    "       Parameters: A python dictionary containing network parameters\n",
    "       x: The input numpy array.\n",
    "       a: Activation of the current state\n",
    "       a_prev: Activation of the previous state.\"\"\"\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    # backprop into h\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] \n",
    "    # backprop through tanh nonlinearity\n",
    "    daraw = (1 - a * a) * da \n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    # initialize your loss to 0\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            # Making one-hot vector\n",
    "            x[t][X[t]] = 1\n",
    "            \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_cell_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    '''Function Parameters: X: The training data example \n",
    "       Y: The training data example.\n",
    "       parameters: The dictionary of parameters'''\n",
    "    # Initialize gradients as an empty dictionary\n",
    "    gradients = {}\n",
    "    \n",
    "    # Retrieve from cache and parameters\n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    \n",
    "    # Backpropagate through time\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    \n",
    "    return gradients, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = gradient_clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, index_to_characters, character_to_indexes, num_iterations = 100 , n_a = 50, baby_names = 100, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates baby names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    baby_names -- number of baby names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, baby_names)\n",
    "    \n",
    "    # Build list of all baby names (training examples).\n",
    "    with open(\"babygirls.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples] # examples is list of all training examples.\n",
    "    \n",
    "    # Shuffle list of all baby names\n",
    "   \n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of RNN\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Loss list.\n",
    "    \n",
    "    # Optimization loop\n",
    "    loss_list = []\n",
    "    loss_list2 = []\n",
    "    for i in range(num_iterations):\n",
    "        total_loss = 0\n",
    "        for j in range(len(examples)):\n",
    "         \n",
    "            index = j \n",
    "       \n",
    "            X = [None] + [character_to_indexes[ch] for ch in examples[index]] \n",
    "            Y = X[1:] + [character_to_indexes[\"\\n\"]]\n",
    "        \n",
    "            # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "            # Choose a learning rate of 0.01\n",
    "            curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        \n",
    "        \n",
    "            # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "            loss = smooth(loss, curr_loss)\n",
    "            #print(loss)\n",
    "            total_loss += loss\n",
    "            loss_list2.append(loss)\n",
    "            \n",
    "            # Every 2000 Iteration, generate \"n\" characters.\n",
    "        if i % 10 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (i, loss) + '\\n')\n",
    "            \n",
    "            # The number of baby names to print\n",
    "            seed = 0\n",
    "            for name in range(baby_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, character_to_indexes)\n",
    "                print_sample(sampled_indices, index_to_characters)\n",
    "            \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "        loss_list.append(total_loss)\n",
    "        \n",
    "    return parameters , loss_list , loss_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 122.491408\n",
      "\n",
      "Lxejeleyniha\n",
      "Ea\n",
      "Aeaenaya\n",
      "Mafitresrannle\n",
      "E\n",
      "Argatana\n",
      "Gci\n",
      "Fdanaa\n",
      "RryxhbmlradoahokahyaAdosnealicaldeelagvaTahialqaigoi\n",
      "Dnims\n",
      "A\n",
      "Jmdssaiy\n",
      "\n",
      "Gse\n",
      "Mba\n",
      "S\n",
      "Iia\n",
      "Sarazgia\n",
      "A\n",
      "Hllee\n",
      "Jge\n",
      "Altkidailyna\n",
      "Yoanre\n",
      "Ma\n",
      "Arraniea\n",
      "\n",
      "Alsa\n",
      "Laie\n",
      "PaalrarerteafamihhaaBierleleantallla\n",
      "Nhmamasaey\n",
      "Ea\n",
      "Nkya\n",
      "Ina\n",
      "\n",
      "Lifmama\n",
      "Tmj\n",
      "Ado\n",
      "Iasjlnjre\n",
      "Lyerle\n",
      "Seqja\n",
      "Oa\n",
      "Sta\n",
      "Da\n",
      "Iiyflsa\n",
      "Da\n",
      "Ebnxiyya\n",
      "Sja\n",
      "Janla\n",
      "Cka\n",
      "Ypa\n",
      "Nye\n",
      "Rfjillcr\n",
      "Lcia\n",
      "Nchnvainionnnseha\n",
      "CnanktmlainaibealvaePateciarsrdalzoneea\n",
      "M\n",
      "Leen\n",
      "A\n",
      "Mahyplrao\n",
      "Ta\n",
      "Alsahsyyele\n",
      "Oanraie\n",
      "Orin\n",
      "Aliokila\n",
      "Aslibalatpa\n",
      "Ia\n",
      "Yerdmanjmewyenure\n",
      "Ye\n",
      "Ayiam\n",
      "Rira\n",
      "Nrjnlsyajana\n",
      "Yhr\n",
      "\n",
      "Rt\n",
      "Miaei\n",
      "Jot\n",
      "Renlslayr\n",
      "ZadsnlhdsrveanmiiqorA\n",
      "Piahljrdlieelana\n",
      "Gha\n",
      "M\n",
      "Mihnmar\n",
      "\n",
      "Inia\n",
      "Mwa\n",
      "Zrn\n",
      "\n",
      "I\n",
      "Alera\n",
      "Fmyenrlmoke\n",
      "Qazi\n",
      "Ga\n",
      "Ndrn\n",
      "Nanadsla\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 10, Loss: 14.916606\n",
      "\n",
      "Waelaclah\n",
      "Caebea\n",
      "Rkhelya\n",
      "Mirlandana\n",
      "Syevvabetta\n",
      "Faimen\n",
      "Rvajaca\n",
      "Courenn\n",
      "Missa\n",
      "Marey\n",
      "Raree\n",
      "Aireton\n",
      "Svirna\n",
      "Phbetel\n",
      "Laliscia\n",
      "Elania\n",
      "Rlenjemtarena\n",
      "Voriet\n",
      "Miten\n",
      "Jaii\n",
      "Miliaggathy\n",
      "Leta\n",
      "Kiyla\n",
      "Peie\n",
      "Marmeszsysyn\n",
      "Myckazdadrigba\n",
      "Mila\n",
      "Lery\n",
      "Areriva\n",
      "Miki\n",
      "Sijabgie\n",
      "Malsni\n",
      "Mirlet\n",
      "Liella\n",
      "Meisox\n",
      "Ria\n",
      "Cagolitgia\n",
      "Raer\n",
      "Erlen\n",
      "Mateni\n",
      "Carienn\n",
      "Jessiale\n",
      "Marra\n",
      "Mizcsh\n",
      "Sililli\n",
      "Lanavola\n",
      "Ayehsala\n",
      "Scmardi\n",
      "Marina\n",
      "Cararia\n",
      "Arynse\n",
      "Rracicyn\n",
      "Ilemsasarabria\n",
      "Rurcia\n",
      "Zuriast\n",
      "Merana\n",
      "Adans\n",
      "Zuemir\n",
      "Avisssa\n",
      "Koica\n",
      "Ialee\n",
      "Lielly\n",
      "Jurlas\n",
      "Iayda\n",
      "Bri\n",
      "Jupbliryn\n",
      "Sokijurgta\n",
      "Jaoliatmadacelyna\n",
      "Celen\n",
      "Joka\n",
      "Ami\n",
      "Amarha\n",
      "Eagketan\n",
      "Kaedah\n",
      "Era\n",
      "Riaida\n",
      "Cerme\n",
      "Alia\n",
      "Vissfiana\n",
      "Aha\n",
      "Eleeth\n",
      "Madya\n",
      "Auede\n",
      "Kislyn\n",
      "Krinry\n",
      "Cethle\n",
      "Myanh\n",
      "Fivia\n",
      "Trina\n",
      "Vmariet\n",
      "Mibmen\n",
      "Eliavcora\n",
      "Aricabaomel\n",
      "Ivserane\n",
      "Arelija\n",
      "Jabse\n",
      "Rissa\n",
      "Iosdestal\n",
      "Maisle\n",
      "Ralesczsabelsoe\n",
      "\n",
      "\n",
      "Iteration: 20, Loss: 13.962504\n",
      "\n",
      "Haimit\n",
      "Aia\n",
      "Mynara\n",
      "Aitana\n",
      "Leletty\n",
      "Elly\n",
      "Audyn\n",
      "Kende\n",
      "Aiabkamerdety\n",
      "Jiegsy\n",
      "Rinah\n",
      "Citalerta\n",
      "Erlias\n",
      "Luyr\n",
      "Rima\n",
      "Kaice\n",
      "Koa\n",
      "Vanze\n",
      "Brie\n",
      "Kanina\n",
      "Mccisy\n",
      "Rupel\n",
      "Linte\n",
      "Kaerla\n",
      "Kiavry\n",
      "Broka\n",
      "Ciisa\n",
      "Kilahs\n",
      "Eite\n",
      "Mulioe\n",
      "Kissta\n",
      "Carda\n",
      "Caiaisalery\n",
      "Maiia\n",
      "Ana\n",
      "Malannth\n",
      "Drora\n",
      "Juccivora\n",
      "Embrye\n",
      "Riiszie\n",
      "Muctia\n",
      "Marla\n",
      "Mielaztia\n",
      "Auia\n",
      "Mori\n",
      "Elee\n",
      "Elie\n",
      "Jula\n",
      "Eria\n",
      "Reya\n",
      "Rid\n",
      "Masha\n",
      "Penmyna\n",
      "Kinssat\n",
      "Elaiy\n",
      "Mina\n",
      "Arlyn\n",
      "Kalina\n",
      "Kriana\n",
      "Chadtca\n",
      "Goria\n",
      "Asam\n",
      "Cirolen\n",
      "Jundyene\n",
      "Malie\n",
      "Mudelkes\n",
      "Kaicela\n",
      "Mijano\n",
      "Arie\n",
      "Misoalena\n",
      "Riaga\n",
      "Caia\n",
      "Kielia\n",
      "Erie\n",
      "Lila\n",
      "Jolie\n",
      "Ristine\n",
      "Jular\n",
      "Crisyn\n",
      "Kata\n",
      "Viedaria\n",
      "Civera\n",
      "Ericmarara\n",
      "Kuise\n",
      "Melond\n",
      "Aun\n",
      "Mensya\n",
      "Imie\n",
      "Midti\n",
      "Brislenrian\n",
      "Rista\n",
      "Ellyn\n",
      "Eliya\n",
      "Curlin\n",
      "Gaisan\n",
      "Aia\n",
      "Ssist\n",
      "Erara\n",
      "Iugh\n",
      "Mariyn\n",
      "\n",
      "\n",
      "Iteration: 30, Loss: 13.620731\n",
      "\n",
      "Erah\n",
      "Arelyn\n",
      "Bran\n",
      "Brina\n",
      "Miane\n",
      "Kaitrprilye\n",
      "Keita\n",
      "Cokikichagitime\n",
      "Renensken\n",
      "Gerlisley\n",
      "Mandelyn\n",
      "Rigia\n",
      "Audiy\n",
      "Miahenra\n",
      "Giahlea\n",
      "Midaleygmiviychalie\n",
      "Miaha\n",
      "Rispwevy\n",
      "Eyeren\n",
      "Erajayari\n",
      "Marie\n",
      "Rym\n",
      "Meqgite\n",
      "Kyah\n",
      "Arleithaciryna\n",
      "Moranda\n",
      "Juola\n",
      "Jiea\n",
      "Kendia\n",
      "Bria\n",
      "Mukiassah\n",
      "Ria\n",
      "Kaia\n",
      "Aranzisgtterrasica\n",
      "Kyanlesiana\n",
      "Morarlyn\n",
      "MinitalenzomaraleiseCyaiedre\n",
      "Erlee\n",
      "Miahana\n",
      "Ricsvir\n",
      "Merith\n",
      "Joulish\n",
      "Suris\n",
      "Gipandratriana\n",
      "Brisgiy\n",
      "Siciserisa\n",
      "Mielekte\n",
      "Essscrasesina\n",
      "Vyalela\n",
      "Rialla\n",
      "Miana\n",
      "Arolie\n",
      "Aphalya\n",
      "Eisicah\n",
      "Lensorryn\n",
      "Mebanna\n",
      "Maclyce\n",
      "Marletgie\n",
      "Muia\n",
      "Miarlyn\n",
      "Ieloa\n",
      "Miria\n",
      "Sia\n",
      "Bynaisish\n",
      "Vynotza\n",
      "Reitlin\n",
      "Erleste\n",
      "Ientciellad\n",
      "Aritty\n",
      "Roarana\n",
      "Jasorjighancrie\n",
      "Kaia\n",
      "Myenny\n",
      "Diandre\n",
      "Gautrellileina\n",
      "Mayalela\n",
      "Atelly\n",
      "Iadanpria\n",
      "Iadanda\n",
      "Akeerie\n",
      "Saillenta\n",
      "Hailly\n",
      "Jowittenanra\n",
      "Mauristchara\n",
      "Adria\n",
      "Moriath\n",
      "Saoladrelyn\n",
      "Sienie\n",
      "Rinessbeit\n",
      "Mellikye\n",
      "Miege\n",
      "Marita\n",
      "Pylah\n",
      "Myn\n",
      "Era\n",
      "Mia\n",
      "Eleison\n",
      "Lesletthaysame\n",
      "Sienina\n",
      "\n",
      "\n",
      "Iteration: 40, Loss: 13.307147\n",
      "\n",
      "Milei\n",
      "Erana\n",
      "Kyanan\n",
      "Melaya\n",
      "Meaie\n",
      "Martina\n",
      "Cidroa\n",
      "Cemoleaza\n",
      "Boniy\n",
      "Vialeah\n",
      "Eland\n",
      "Drielladrenna\n",
      "Viprianne\n",
      "Maney\n",
      "Mellana\n",
      "Resolllia\n",
      "Dertie\n",
      "Assda\n",
      "Madrylla\n",
      "Jertie\n",
      "Melaitoz\n",
      "Kearemoryana\n",
      "Juli\n",
      "Rentya\n",
      "Keana\n",
      "Kalen\n",
      "Apalia\n",
      "Briana\n",
      "Oslerina\n",
      "Morlyn\n",
      "Beristkarie\n",
      "Cterissi\n",
      "Endrey\n",
      "Irilie\n",
      "Sliolia\n",
      "Marlec\n",
      "Micalie\n",
      "Resty\n",
      "Melgilie\n",
      "Caitlity\n",
      "Chevcithia\n",
      "Uia\n",
      "Avaya\n",
      "Mocizya\n",
      "Iaha\n",
      "Riancate\n",
      "Mikatria\n",
      "Asa\n",
      "Miaena\n",
      "Ambrielyra\n",
      "Aliana\n",
      "Egia\n",
      "Selye\n",
      "Karlettarsan\n",
      "Kaislie\n",
      "Elbya\n",
      "Bmiem\n",
      "Melaya\n",
      "Maranni\n",
      "Menitadahcamira\n",
      "Dalia\n",
      "Isitha\n",
      "Iza\n",
      "Likailia\n",
      "Eutryna\n",
      "Kalia\n",
      "Ardan\n",
      "Meris\n",
      "Kiniyah\n",
      "Veryaha\n",
      "Rodie\n",
      "Keritt\n",
      "Merania\n",
      "Ryley\n",
      "Jeikaina\n",
      "Keadandy\n",
      "Elizsa\n",
      "Micielyn\n",
      "Alielicperrya\n",
      "Suvia\n",
      "Cina\n",
      "Maiter\n",
      "Brietzyl\n",
      "Caig\n",
      "Amaizlea\n",
      "Pickyna\n",
      "Ciszana\n",
      "Briana\n",
      "Byli\n",
      "Kuolie\n",
      "Esana\n",
      "Alaalena\n",
      "Maconra\n",
      "Elinee\n",
      "Mauie\n",
      "Assanel\n",
      "Asskrec\n",
      "Midahk\n",
      "Miilaa\n",
      "Elah\n",
      "\n",
      "\n",
      "Iteration: 50, Loss: 13.169860\n",
      "\n",
      "Sausov\n",
      "Coina\n",
      "Amiram\n",
      "Surali\n",
      "Eubrich\n",
      "Aurra\n",
      "Alenta\n",
      "Amargi\n",
      "Mrie\n",
      "Raala\n",
      "Evie\n",
      "Ceisyn\n",
      "Elian\n",
      "Melaka\n",
      "Coaryan\n",
      "Ana\n",
      "Aiddrylesa\n",
      "Elissry\n",
      "Mendy\n",
      "Eliryn\n",
      "Ensisactaliah\n",
      "Siganze\n",
      "Keie\n",
      "Viqdina\n",
      "Iosan\n",
      "Linsh\n",
      "Kailorsdros\n",
      "Asie\n",
      "Admiza\n",
      "Malielia\n",
      "Jsartot\n",
      "Delie\n",
      "Brielia\n",
      "Cucelez\n",
      "Aivline\n",
      "Micrikdy\n",
      "Asi\n",
      "Poudyabetgrene\n",
      "Delee\n",
      "Kaira\n",
      "Monevies\n",
      "Aridic\n",
      "Asmynn\n",
      "Koa\n",
      "Elinitjelia\n",
      "Eizly\n",
      "Eegan\n",
      "Siavana\n",
      "Vivian\n",
      "Elictty\n",
      "Rivi\n",
      "Melana\n",
      "Kailabe\n",
      "Aliyah\n",
      "Isza\n",
      "Keith\n",
      "Elmokyn\n",
      "Maley\n",
      "Pizorie\n",
      "Amlitt\n",
      "Miase\n",
      "Jopiyla\n",
      "Vie\n",
      "Minova\n",
      "Sylron\n",
      "Mena\n",
      "Asaelyn\n",
      "Jianda\n",
      "Baliszenne\n",
      "Miana\n",
      "Aloe\n",
      "Mincie\n",
      "Mikrasa\n",
      "Paisyn\n",
      "Meida\n",
      "Medist\n",
      "Malla\n",
      "Elian\n",
      "Ana\n",
      "Ariy\n",
      "Eledi\n",
      "Briara\n",
      "Aulazie\n",
      "Elileighaka\n",
      "Gissinocelarlah\n",
      "Modemi\n",
      "Elanna\n",
      "Siadany\n",
      "Mala\n",
      "Essahe\n",
      "Veistatliroot\n",
      "Mizlynn\n",
      "Amirana\n",
      "Miama\n",
      "Jousyn\n",
      "Salintt\n",
      "Julicic\n",
      "Ivani\n",
      "Maiey\n",
      "Zaley\n",
      "\n",
      "\n",
      "Iteration: 60, Loss: 12.949928\n",
      "\n",
      "Gaary\n",
      "Iszen\n",
      "Bainktily\n",
      "Mastie\n",
      "Klecyan\n",
      "Elian\n",
      "Jociy\n",
      "Miavka\n",
      "Kentalia\n",
      "Monda\n",
      "Media\n",
      "Ryelona\n",
      "Maisalnny\n",
      "Berliste\n",
      "Ara\n",
      "Monny\n",
      "Amilah\n",
      "Vianinoxaliana\n",
      "Morith\n",
      "Aurled\n",
      "Joada\n",
      "Ara\n",
      "Maicle\n",
      "Ricia\n",
      "Elylam\n",
      "Ara\n",
      "Maisdie\n",
      "Miisdy\n",
      "Miena\n",
      "Mailey\n",
      "Mocdy\n",
      "Giviade\n",
      "Menielm\n",
      "Mikei\n",
      "Soie\n",
      "Sevie\n",
      "Alisza\n",
      "Ilani\n",
      "Jloanna\n",
      "Eleettylia\n",
      "Auli\n",
      "Sicelyn\n",
      "Jyliot\n",
      "Radi\n",
      "Mirontanyelei\n",
      "Kordana\n",
      "Rialaia\n",
      "Maelyn\n",
      "Macila\n",
      "Ercimledy\n",
      "Smileah\n",
      "Mondena\n",
      "Kyleia\n",
      "Melicma\n",
      "Mara\n",
      "Semin\n",
      "Elenton\n",
      "Giderzyra\n",
      "Muerta\n",
      "Iy\n",
      "Garsam\n",
      "Isia\n",
      "Mena\n",
      "Moestyna\n",
      "Koriya\n",
      "Rlivia\n",
      "Alerina\n",
      "Janysa\n",
      "Maer\n",
      "Aulary\n",
      "Amalina\n",
      "Bemidy\n",
      "Meevie\n",
      "Ara\n",
      "Aviley\n",
      "Inabestelde\n",
      "Brice\n",
      "Elan\n",
      "Maine\n",
      "Monyan\n",
      "Brrath\n",
      "Isabert\n",
      "Flizory\n",
      "Mila\n",
      "Miaberi\n",
      "Angyel\n",
      "Versan\n",
      "Muntei\n",
      "Kayi\n",
      "Aulicslikallejodia\n",
      "Mya\n",
      "Brylana\n",
      "Madriy\n",
      "Merbera\n",
      "Aliy\n",
      "Araya\n",
      "Veiva\n",
      "Alianna\n",
      "Moriah\n",
      "Dipa\n",
      "\n",
      "\n",
      "Iteration: 70, Loss: 12.844917\n",
      "\n",
      "Mikoia\n",
      "Andisly\n",
      "Olaidc\n",
      "Gelietanrie\n",
      "Brinia\n",
      "Alie\n",
      "Hidena\n",
      "Mielia\n",
      "Anserana\n",
      "Evany\n",
      "Rowio\n",
      "Ela\n",
      "Alisa\n",
      "Apissmelya\n",
      "Mena\n",
      "Dylanna\n",
      "Cuitnic\n",
      "Glee\n",
      "Anabkais\n",
      "Seigoh\n",
      "Ruel\n",
      "Alexza\n",
      "Aupy\n",
      "Seiznit\n",
      "Erinlessy\n",
      "Cammee\n",
      "Aranda\n",
      "Denancy\n",
      "Briatan\n",
      "Evey\n",
      "Cumiana\n",
      "Clei\n",
      "Jeiceth\n",
      "Beiasah\n",
      "Caitlia\n",
      "Awalistiry\n",
      "Erin\n",
      "Alissta\n",
      "Cenryna\n",
      "Auarie\n",
      "Mmignon\n",
      "Crinon\n",
      "Aidaise\n",
      "Minlia\n",
      "Amisa\n",
      "Minton\n",
      "Keivine\n",
      "Riaven\n",
      "Aurlinna\n",
      "Avanne\n",
      "Felliny\n",
      "Anexine\n",
      "Minone\n",
      "Drivia\n",
      "Arsigere\n",
      "Elista\n",
      "Reielve\n",
      "Jurlia\n",
      "Elia\n",
      "Rorttia\n",
      "Satrole\n",
      "Barlra\n",
      "Alianna\n",
      "Moev\n",
      "Ricten\n",
      "Amalia\n",
      "Amiga\n",
      "Rileos\n",
      "Avelex\n",
      "Morixze\n",
      "Driama\n",
      "Isabryn\n",
      "Avelyn\n",
      "Eliva\n",
      "Kendyn\n",
      "Ersitolie\n",
      "Driana\n",
      "Kidia\n",
      "Maienne\n",
      "Kuranilyn\n",
      "Suitz\n",
      "Alicley\n",
      "Orilee\n",
      "Monty\n",
      "Gilana\n",
      "Rlisalii\n",
      "Anaya\n",
      "Arida\n",
      "Mistt\n",
      "Meevi\n",
      "Mikler\n",
      "Moue\n",
      "Milasse\n",
      "Audyn\n",
      "Eliana\n",
      "Alieson\n",
      "Anati\n",
      "Beilan\n",
      "Bersin\n",
      "Vira\n",
      "\n",
      "\n",
      "Iteration: 80, Loss: 12.647873\n",
      "\n",
      "Elagica\n",
      "Keigha\n",
      "Ciliela\n",
      "Kaille\n",
      "Fsier\n",
      "Abristh\n",
      "Rielle\n",
      "Mendy\n",
      "Koora\n",
      "Erana\n",
      "Cislie\n",
      "Millia\n",
      "Rauria\n",
      "Jonder\n",
      "Arandy\n",
      "Paite\n",
      "Meisyn\n",
      "Ieeska\n",
      "Erinith\n",
      "Sserynn\n",
      "Anilla\n",
      "Jala\n",
      "Mictaleda\n",
      "Eara\n",
      "Cerla\n",
      "Eliana\n",
      "Mera\n",
      "Ckandhaa\n",
      "Mour\n",
      "Armazy\n",
      "Arlotte\n",
      "Aralyn\n",
      "Villendn\n",
      "Elemine\n",
      "Erona\n",
      "Rieleigh\n",
      "Menel\n",
      "Krisdenne\n",
      "Monsseste\n",
      "Monet\n",
      "Milente\n",
      "Arilyn\n",
      "Aemyn\n",
      "Gikia\n",
      "Ariy\n",
      "Erino\n",
      "Malie\n",
      "Baroa\n",
      "Mengen\n",
      "Keityn\n",
      "Jallyn\n",
      "Esmigh\n",
      "Katalupellia\n",
      "Vomina\n",
      "Aramz\n",
      "Yaelyn\n",
      "Erivy\n",
      "Keary\n",
      "Erive\n",
      "Rianra\n",
      "Anlir\n",
      "Ceileny\n",
      "Mara\n",
      "Erimy\n",
      "Cian\n",
      "Moitnia\n",
      "Ariee\n",
      "Meanch\n",
      "Jola\n",
      "Eitlie\n",
      "Madina\n",
      "Alicna\n",
      "Briny\n",
      "Menna\n",
      "Emida\n",
      "Erisyah\n",
      "Moresifelina\n",
      "Mintardrratza\n",
      "Sethelyna\n",
      "Elvelyn\n",
      "Maerine\n",
      "Monie\n",
      "Doranna\n",
      "Maajizelyn\n",
      "Corbyne\n",
      "Mokien\n",
      "Milonti\n",
      "Aitra\n",
      "Monet\n",
      "Meitynd\n",
      "Meignet\n",
      "Meelise\n",
      "Arilya\n",
      "Erineet\n",
      "Diless\n",
      "Erdande\n",
      "Peittie\n",
      "Elie\n",
      "Sie\n",
      "Erolene\n",
      "\n",
      "\n",
      "Iteration: 90, Loss: 12.692060\n",
      "\n",
      "Eliz\n",
      "Keiga\n",
      "Gissjena\n",
      "Alissa\n",
      "Vivie\n",
      "Recie\n",
      "Audlivla\n",
      "Kesley\n",
      "Elynn\n",
      "Virten\n",
      "Alicanem\n",
      "Gelina\n",
      "Kena\n",
      "Meni\n",
      "Keitlect\n",
      "Morie\n",
      "Milattraub\n",
      "Meitnn\n",
      "Hiara\n",
      "Leellie\n",
      "Julieth\n",
      "Ela\n",
      "Aulow\n",
      "Medella\n",
      "Mordyn\n",
      "Vilgh\n",
      "Morelyn\n",
      "Elana\n",
      "Durleigh\n",
      "Elmorh\n",
      "Hiana\n",
      "Koit\n",
      "Elici\n",
      "Elis\n",
      "Alann\n",
      "Keiga\n",
      "Maita\n",
      "Eliy\n",
      "Ellia\n",
      "Kyna\n",
      "Ellynn\n",
      "Elani\n",
      "Bui\n",
      "Mera\n",
      "Alicia\n",
      "Imaro\n",
      "Elana\n",
      "Kerandencelle\n",
      "Britse\n",
      "Mailei\n",
      "More\n",
      "Matalia\n",
      "Mylelest\n",
      "Sira\n",
      "Meirah\n",
      "Alite\n",
      "Artyna\n",
      "Eurtelyly\n",
      "Andra\n",
      "Eliande\n",
      "Riea\n",
      "Anare\n",
      "Berley\n",
      "Merana\n",
      "Mielin\n",
      "Kena\n",
      "Esana\n",
      "Marataeeria\n",
      "Rivey\n",
      "Meanot\n",
      "Amirza\n",
      "Moni\n",
      "Meydana\n",
      "Eutha\n",
      "Ilie\n",
      "Esindtt\n",
      "Jenie\n",
      "Kyle\n",
      "Alana\n",
      "Angie\n",
      "Soia\n",
      "Toke\n",
      "Koine\n",
      "Ayler\n",
      "Menitc\n",
      "Doen\n",
      "Mecandyn\n",
      "Ticrea\n",
      "Beie\n",
      "Meennd\n",
      "Briste\n",
      "Sulot\n",
      "Milens\n",
      "Ceranda\n",
      "Tanlee\n",
      "Kace\n",
      "Riegrittar\n",
      "Elia\n",
      "Roria\n",
      "Marithae\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters , loss_list , loss_list2  = model(names, index_to_characters, character_to_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cost of an Example')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111637588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list2)\n",
    "plt.xlabel(\"Number of Stochastic Updates\")\n",
    "plt.ylabel(\"Cost of an Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total cost of all examples in an epoch')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xu8XGV97/HPd265QbITiIiEGBVQ\nwQpCRFrxUuzhVs8BrRyhKnlZWtSi1XorWHtobbVar6WHUhUQaFXkeDlii2KMeDsqEAhXEYmIEAUS\nCLlAkn2Z+Z0/1jN7T3ZmZq9k77U3s/f3/cq8ZtYza9b6rZmd+c3zrGc9jyICMzOzIpWmOgAzM5v+\nnGzMzKxwTjZmZlY4JxszMyuck42ZmRXOycbMzArnZGNmZoVzsjEzs8I52ZiZWeEqUx3Ak8W+++4b\ny5Ytm+owzMx6yk033fRIRCweaz0nm2TZsmWsXr16qsMwM+spkn6dZz03o5mZWeGcbMzMrHBONmZm\nVjgnGzMzK5yTjZmZFc7JxszMCudkY2ZmhXOyGadVdz3MRd/75VSHYWb2pOZkM07f/8UGPvMDJxsz\ns26cbMapWi4xWI+pDsPM7EnNyWacquUSA/XGVIdhZvak5mQzTrWyGKw3iHDtxsysEyebcaqWS0RA\nveFkY2bWiZPNOFUr2VvopjQzs85yTTEg6anA0tb1I+LHRQXVS6rlLNkMDgXUpjgYM7MnqTGTjaQP\nAa8Hfg7UU3EAJxcYV8+ouWZjZjamPDWbPwIOiYgdRQfTi2plATDoZGNm1lGecza/yrnejDTcjOZk\nY2bWUceajaRPkjWXbQXWSPoO0N98PiLeWXx4T35ONmZmY+vWjHZHur8T+NYkxNKTmsmmf8jJxsys\nk47JJiIuAZA0GxiIiEZaLuF+V8NqleY5G19nY2bWSZ5zMdcB81qW5wHfLSac3uNmNDOzseVJNnMi\nYmtzIT2eW1xIvaU2fJ2Nk42ZWSd5ks02SYc3FyQdAbgbdOIRBMzMxpbnOpu/BL4m6ddpeSlwRnEh\n9Zbhmo3P2ZiZdTRmsomI6yU9F3guIODOiBgoPLIe4XM2ZmZjyzNcTQU4C3hpKvqepIsjYqjQyHpE\nNY0gMOBzNmZmHeVpRruQrAfapWn59cCRwNlFBdVLmjUbn7MxM+ssT7I5JiIOb1n+tqRbiwqo18yq\nuBnNzGwseXqjNSQtay6kx/5mTaru+mxmNqY8NZv3Aj+Q9AuyDgIHkZ3DMUa6Prs3mplZZ2PWbCJi\nJfBssqTzXuA5EfGdsV4n6UBJ10m6S9Kdkt6eyhdJWinpnnS/MJVL0gWS1kq6TdKRLdtakda/R9KK\nlvKjJN2eXnOBJHXbRxGGOwi4Gc3MrKMxk42kWcCfAeeSJZuzUtlYhoB3RcRzgWOAcyQdmrazKiIO\nBlalZYCTgIPT7WzgorT/RcD5wIuAo4HzW5LHRWnd5utOTOWd9jHhqqXUQcDNaGZmHeU5Z3M5cBTw\nWeBisp5ol4/1ooh4MCJuTo+3AncBBwCntLz+cuDU9PgU4IrI/BTok7Q/cAKwMiI2RsRjwErgxPTc\n/Ij4SUQEcMWobbXbx4QrlUSlJHcQMDPrIs85m0Mj4vktyyt3tzda6lTwAuB6YL+IeBCyhCTpKWm1\nA4AHWl62LpV1K1/Xppwu+xgd19mkLtxLly7dnUPaSbVccrIxM+siT83mFkkvbC5IOgr4Sd4dSNoL\n+ArwjojY0m3VNmWxB+W5RcRnImJ5RCxfvHjx7rx0J7VKyR0EzMy6yJNsjgSuTyfh1wI3AL8naY2k\nm7u9UFKVLNF8PiK+moofTk1gpPv1qXwdcGDLy5cAvx2jfEmb8m77KES1XHIHATOzLvIkm1PITr6f\nkG6HAK8CXgOc1ulFqWfYJcBdEfGJlqeuBpo9ylYAX28pPzP1SjsG2Jyawq4Fjpe0MHUMOB64Nj23\nVdIxaV9njtpWu30UolaWr7MxM+siz0Ccv0xf/odExBWpd9heEXH/GC99MfAG4HZJt6Sy9wEfBq6S\ndBZwPyMJ6xrgZGAtsA14Y9r/Rkl/D9yY1vtARGxMj98CXAbMAb6ZbnTZRyGqFZ+zMTPrJs9AnO8n\nSxzPIuvxNQf4AnBst9dFxI9of14F4BVt1g/gnA7bupSRsdlay1cDz2tT/mi7fRTFzWhmZt3laUZ7\nDVmN4wmAiPgNML/IoHpNtVxiYMgdBMzMOsmTbPpTrSMAJHlK6FFqZV9nY2bWTZ5k81VJFwILJL0R\n+DZtmrRmsprP2ZiZdZWng8BHJJ0EDACHAx+MiG+O8bIZxRd1mpl1l2cEAVJycYLpoFou8cRAfarD\nMDN70srTjGZjyDoIuGZjZtaJk80EqFXcQcDMrBsnmwngczZmZt3luajzGLL5ZJ6e1hfZNZiHFBxb\nz6iVSx6uxsysizwdBD5HNmnaTYDPgrdRrZQY8KjPZmYd5Uk2WyLiG4VH0sNqbkYzM+sqT7L5rqR/\nBL4K9DcLI+K2wqLqMVWPIGBm1lWeZHPsqHvIhq556cSH05vc9dnMrLs8Iwi8ZDIC6WXVcomhRtBo\nBKVSp4GuzcxmrlwjCEg6ATgMmN0si4gPFRVUr6lVsh7kg40Gs0rlKY7GzOzJJ0/X538F+siazT4H\n/BHw04Lj6im1cko29WBWrvRtZjaz5Lmo89iI+GPg0Yj4G+BFwJJiw+ot1XLWdOZrbczM2suTbLan\n+x2SngrsAJYVFlEPqjab0dwjzcysrTyNPt+U1Ad8DLiF7MLOywuNqsdUUzNav2s2ZmZt5emN9rfp\n4f+R9J/AnIjYWGhUPWbknI2TjZlZO7t1OjsitjPSrGZJtaWDgJmZ7cqjPk+Ams/ZmJl15WQzAZq9\n0QacbMzM2sp7UedTgaWt60fEj4sKqtcMn7NxBwEzs7byXNT5IeD1wM8ZmWIggJMLjKunjHR99jkb\nM7N28tRs/gg4JCJ2FB1Mr2p2EBioe7ofM7N28pyz+VXO9Was4XM2Q67ZmJm1k6dmsxVYI+k77Dyf\nzTsLi6rH+DobM7Pu8iSbb6WbdeCuz2Zm3eUZQeCSyQikl1VdszEz66pjspH0xYg4Q9Iast5nO4mI\nIwuNrIeMdBDwORszs3a61Wzek+5fMxmB9LLmORtPDW1m1l7HZBMR69L9LycvnN5UraT5bNyMZmbW\nlrs0T4CqRxAwM+vKyWYCVEqu2ZiZdbNbyUbSAkmHFhVMr5JErVJyBwEzsw7GTDaSVkmaL2khcDvw\nBUkfLT603lIrl1yzMTPrIE/NZlFEbAFeDVweEUcAJ4z1IkmXSlov6Y6Wsr+V9BtJt6TbyS3PnSdp\nraS7JZ3QUn5iKlsr6dyW8mdIul7SPZK+JKmWymel5bXp+WV53ojxqpblZGNm1kGeZFORtBg4DfjG\nbmz7MuDENuWfjIgj0u0agNQ0dzpwWHrNv0oqSyoDFwInAYcCZ7Q0430kbetg4DHgrFR+FvBYRBwE\nfDKtV7hqueSuz2ZmHeRJNh8Evg/cHxE3SHom2eCcXUXED4CNOeM4BbgyIvoj4lfAWuDodFsbEfdG\nxABwJXCKJAHHAV9Or78cOLVlW5enx18GXpHWL1S1XPLkaWZmHYyZbCLiyog4NCLOTsv3RsQp49jn\nWyXdlprZFqayA4AHWtZZl8o6le8DbIqIoVHlO20rPb85rb8LSWdLWi1p9YYNG8ZxSNn4aJ7Pxsys\nvTwdBA6SdK2kW9Py8yWdt4f7uwh4FnAE8CDw8eZu2qwbe1DebVu7FkZ8JiKWR8TyxYsXd4t7TLVy\nydfZmJl1kKcZ7WLg74DmN+ntZDN37raIeDgi6hHRAD5L1kwGWc3kwJZVlwC/7VL+CNAnqTKqfKdt\npecXkL85b49VK+4gYGbWSZ5kMy8iftxciIgABvdkZ5L2b1l8FdDsqXY1cHrqSfYM4GDgBuBG4ODU\n86xG1ong6hTDdYyM27YC+HrLtlakx68BvpvWL5TP2ZiZdZZnPptHUwIIAEmnAg+N9SJJXwReDuwr\naR1wPvBySUekbd0HvAkgIu6UdBXwM2AIOCci6mk7bwWuBcrApRFxZ9rFXwFXSvoHYA3QnArhEuDf\nJa0lq9GcnuMYx63q62zMzDrKk2zeSvYF/hxJvyY713LGWC+KiHbrdJwbJyI+SNbzbXT5NcA1bcrv\nZaQZrrV8B1k37UlVK5fYNjA09opmZjNQnsnT1gLHSVoAKCI2FR9W78ku6nRvNDOzdrpNnvYXHcoB\niIgLCoqpJ7kZzcyss241m/H1BZ5hsoE4nWzMzNrpNnna30xmIL3OA3GamXWW56LOZZK+JumhdPvK\nZA1u2Uuq5RKDQz5nY2bWTp7rbL5Idu3K0nT7RiqzFtWK3IxmZtZBnmRTiojPRcRAul2W83UzStXD\n1ZiZdZTnOpvvSno32YjLAbwW+Iak+QBprpsZr+YRBMzMOsqTbJrjoL19VPmbyJLP0gmNqEdloz47\n2ZiZtZPnos4Dx1rHsma0RkC9EZRLhU+fY2bWU8ZMNpJKZLNnLmtd3xd17qxazk5jDdYblEvlKY7G\nzOzJJU8z2tfJmstuZ2SaARulWs5qMwP1BrOrTjZmZq3yJJtlEfE7hUfS42qVrGYz4B5pZma7yNOF\n+VpJxxUeSY9rbUYzM7Od5anZ/JCsq3MAA2TTLkdELCo0sh4znGw8ioCZ2S7yJJtPAi/B52y6Gm5G\nc83GzGwXeZLNPcCayZhauZfVUgcBN6OZme0qT7L5LdkoAtcA/c1Cd33emc/ZmJl1lifZrEu3+QXH\n0tOayca90czMdpVnBAHPa5PDcLJxzcbMbBd5RhDYF3gXcBgwu1keEccXGFfPqVWa52x8asvMbLQ8\n19n8B3AfcAjwEeAh4JYCY+pJI12fXbMxMxstT7JZHBGfBgYiYhWwAji62LB6T7PrszsImJntKk8H\ngcF0/5CkE8h6p3kk6FF8zsbMrLM8yeZDkhYA7wYuJOuV9p5Co+pBteGuzz5nY2Y2Wp7eaFenh7eR\njSSApGqRQfUid302M+tszHM2klZJWtqyfCRwQ6FR9aCqRxAwM+soTzPaJ4CVkj4OHACcCvxpoVH1\noKo7CJiZdZSnGe2/JD0OfAd4BDgiIh4uPLIeU3MHATOzjvI0o51H1jHgOOAfgO+lXmnWwlMMmJl1\nlqcZbQlwdERsA34o6ZvAJcC1hUbWY8olUS7JzWhmZm3kaUY7B0DSrIjoj4h7gd8vPLIeVC3LzWhm\nZm3kaUY7WtLtZPPaIOlwSf9SeGQ9qFouueuzmVkbeYaruQB4JfAoQETcims2bdXKJTejmZm1kSfZ\nlCLi16PK6kUE0+uqTjZmZm3l6SDwgKSjgZBUBt4G/KLYsHpTrVLycDVmZm3kqdm8BXgnsBR4GDgm\nldko7iBgZtbemMkmItZHxOkRsW+6nR4Rj4z1OkmXSlov6Y6WskWSVkq6J90vTOWSdIGktZJuS0Pi\nNF+zIq1/j6QVLeVHSbo9veYCSeq2j8lQLZc8n42ZWRt5ajZ76jLgxFFl5wKrIuJgYFVaBjgJODjd\nzgYugixxAOcDLyKbQ+f8luRxUVq3+boTx9hH4WqVkms2ZmZtFJZsIuIHwMZRxacAl6fHl5ONs9Ys\nvyIyPwX6JO0PnACsjIiNEfEYsBI4MT03PyJ+EhEBXDFqW+32UTh3EDAza6/Imk07+0XEgwDp/imp\n/ADggZb11qWybuXr2pR320fhqmV5uBozszY69kaT9BfdXhgRF0xgHGq3iz0o372dSmeTNcWxdOnS\nMdYeW61SZsv2wbFXNDObYbrVbBaPcdsTD6cmMNL9+lS+jp2nml5CNv10t/Ilbcq77WMXEfGZiFge\nEcsXL97TQxpRK3tsNDOzdjrWbCLibwrY39XACuDD6f7rLeVvlXQlWWeAzRHxoKRryaalbnYKOB44\nLyI2Stoq6RjgeuBM4F/G2EfhfM7GzKy9bs1on+j2woh4Z7fnJX0ReDmwr6R1ZL3KPgxcJeks4H7g\ntLT6NcDJwFpgG/DGtI+Nkv4euDGt94GIaHY6eAtZj7c5wDfTjS77KJzHRjMza6/bCAJ3jmfDEXFG\nh6de0WbdAM7psJ1LgUvblK8Gntem/NF2+5gMWc3GHQTMzEbr1ox2yWQGMh3UKh5BwMysnTHHRpO0\nL/Au4DBgdrM8Io4vMK6e5HM2Zmbt5bnO5j+A+4BDgI8ADwG3FBhTz6p5uBozs7byJJvFEfFpYCAi\nVpH18Dq62LB6U9WjPpuZtZVnioHmVYoPSTqB7HqWA7usP2PNrpQZqDcYrDeolid7cAYzsyevPMnm\nQ5IWAO8GLgTmA+8pNKoe1Te3CsCW7YPss9esKY7GzOzJY8xkExFXp4e3AS8pNpze1kw2j21zsjEz\na+W2ngm0YE6WbDZvH5jiSMzMnlycbCbQwrk1ADZt82CcZmatOiYbSW9N98dMXji9rdmM5mRjZraz\nbjWbP033/zoZgUwHfXNSzcbTDJiZ7aRbB4FfSFoL7C/p5pZykQ1ndmSxofWevWdXkGDTNp+zMTNr\n1W1stP8paQlwLZM4cnIvK5XEgjlVN6OZmY3StetzRKwDDpNUAQ5KxWsjYqjwyHrUwrk1N6OZmY2S\nZyDOY8nGR/sNWRPaUyW9ISL+X9HB9aKsZuNmNDOzVnlGEPgUcHJE/AxA0nOBfweWFxlYr+qbW+XR\nx51szMxa5bnOptZMNAARcRdQKy6k3tY3p8omX9RpZraTPDWbmyV9mqw2A/A6YE1xIfW2vrk1dxAw\nMxslT83mzcAvgfcCfwXcC7ypyKB6Wd/cKlt3DDHkSdTMzIblGYhzB/BP6WZj6Evjo23ZMcSieW5t\nNDMDj4024frS+GiPuUeamdkwJ5sJtsDjo5mZ7WLMZCPp1XnKLNMc+dnTDJiZjchTs3l/m7K/nuhA\npovmORvXbMzMRnTsICDpBOBE4ABJn2h5aj7grlYdeJoBM7NddeuNth64A9gB3NlSvhU4t8igetne\ns6se+dnMbJRuoz6vAdZI+nzq/oykBcABEfHIZAXYa8rNkZ89GKeZ2bA852z+S9J8SQuB24EvSPpo\nwXH1tD5PM2BmtpM8yWZRRGwBXg1cDrwAOKHQqHrcAk8zYGa2kzzJpiJpMdkEat+IiCg4pp7X52kG\nzMx2kifZfBD4PvBARNwg6ZnAr4oNq7f1zXUzmplZqzxjo10JXNmyfC9wSpFB9bqFc2uu2ZiZtcgz\ngsDTJF0l6cF0+5Kkp01GcL1qwZwqW3YMUW+4xdHMDPI1o30OWAksS7eVqcw6aF7YucWdBMzMgHzJ\nZr+I+GxE9KfbxcB+RQfWy5rJxiM/m5ll8iSbjZJO14jXAhuLDqyXNacZcPdnM7NMnmTzJ8CZwCPA\nBuANwFlFBtXrmoNxbnaPNDMzIEeyiYj7IuLkiNgnIvaNiFdGxLi6Pku6T9Ltkm6RtDqVLZK0UtI9\n6X5hKpekCyStlXSbpCNbtrMirX+PpBUt5Uel7a9Nr9V44t1dIzUbN6OZmUG+3miXSOprWV4o6bMT\nsO/fj4gjImJ5Wj4XWBURBwOrGBns8yTg4HQ7G7goxbEIOB94EXA0cH4zQaV1zm553YkTEG9unmbA\nzGxneZrRjoyITc2FiHgMOKqAWE4hGw6HdH9qS/kVkfkp0Cdpf7Ihc1ZGxMYU00rgxPTc/Ij4SRrt\n4IqWbU2K+XOykZ8fc7IxMwPyJZtSGu0ZyGo2QHWc+w3g25JuknR2KtsvIh4ESPdPSeUHAA+0vHZd\nKutWvq5N+aQpl8T82VU2uzeamRmQYwQB4FPATyR9iSxJnA780zj3++KI+K2kpwArJf28y7rtzrfE\nHpTvuuEs0Z0NsHTp0u4R76a+uZ5mwMysKU8Hgc+RJZjNZBOnvTYiLhvPTiPit+l+PfA1snMuD6cm\nMNL9+rT6OuDAlpcvAX47RvmSNuXt4vhMRCyPiOWLFy8ezyHtwtMMmJmNyNOMRkTcFhGfiohPRsTt\n49mhpHmS9m4+Bo4nmxH0aqDZo2wF8PX0+GrgzNQr7Rhgc2pmuxY4PnVYWJi2c216bqukY1IvtDNb\ntjVpFnh8NDOzYXma0SbafsDXUm/kCvCFiPiWpBuBqySdBdxPNqUBwDXAycBaYBvwRoCI2Cjp74Eb\n03ofiIjmxaZvAS4D5gDfTLdJtXBulV8/+sRk79bM7Elp0pNNGjX68DbljwKvaFMewDkdtnUpcGmb\n8tXA88Yd7Di4Gc3MbESuZjTbfQvm1tiyY9AjP5uZ0aVmI+kx2vfiElmFY1FhUU0D++5VIwIeebyf\n/ebPnupwzMymVLdmtH0nLYpp6Ln7zwfg9nWb2e9QJxszm9k6NqNFRL31BiwgO7nfvFkXhz1tPiXB\nres2jb2ymdk0l2dstD+U9Auy61euT/ffLTqwXje3VuGQ/fbm1nWbpzoUM7Mpl6eDwAeBFwN3R8SB\nZGOSfa/IoKaLw5f0cdu6TWQd6szMZq48yWYoIjaQjZGmiFgJHDnWiwwOP7CPTdsGuX/jtqkOxcxs\nSuW5zmZzutL/R8AVktYDjWLDmh6evyQbv/TWdZt5+j7zpjgaM7Opk6dmcyqwA3gHWfPZb4BXFhjT\ntPHsp+7NrEqJWx9wJwEzm9nyJJvzUo+0wYi4JCI+Abyz6MCmg2q5xGFPm89t7pFmZjNcnmTTbpbL\nP5zoQKar5y/p447fbGGo7pZHM5u5OiYbSW+StAZ4tqSbW273AD+bvBB72+EHLmD7YJ171j8+1aGY\nmU2Zbh0ErgJWAf8InNtSvjXNQ2M5HL6kD4Db1m0aHlXAzGym6TaCwGMRsTYiTiMbqv+/pdvEzjI2\nzS3bZx57z65wywO+uNPMZq48IwicQ1bLWZpuV0n686IDmy5KJfH8JQvcScDMZrQ8HQTeBBwdEe+L\niPcBLwLeXGxY08vhS/q4+6Gt7BisT3UoZmZTIk+yEdA6C9hgKrOcjj1oX4Yaweevv3+qQzEzmxLd\neqM1Ow/8O/BTSe+X9H7gx8DlkxHcdPG7z9qHlx2ymE+t/AWPPN4/1eGYmU26bjWbGwAi4p+As4Ft\nwHbgzRHxsUmIbdqQxP/674eyfbDOR79191SHY2Y26bp1fR5uKouIG4Ebiw9n+nrW4r34k2OfwWd/\neC9//KKlHH5g31SHZGY2abolm8WSOg5Lk4atsd3wtuMO4mtrfsP5V9/JV97ye5RLPvVlZjNDt2a0\nMrAXsHeHm+2mvWdXOe+k53DLA5s47d9+zNr1W6c6JDOzSdGtZvNgRHxg0iKZIV71ggOQ4O++8TNO\n/ucf8bbjDuL1xzydhfNqUx2amVlh1GkWSUlrIuIFkxzPlFm+fHmsXr160vb3yOP9nH/1nfzXbQ8i\nZdfivOyQxRz2tPk8c/FeLF00l1olT890M7OpI+mmiFg+5npdks2iiNg44ZE9SU12smm6bd0mvvvz\n9Xzv7g3cum4TzY+jXBL7zKuxeO9ZLN57Fovm1lgwt0rfnBoL5lTYe3aVvWdX2Gt2hb1mVZg3a+R+\nbrVMyeeDzGwSjDvZzDRTlWxabdkxyL0bnuDeDY9z3yNP8PCWfjY83s+Grf08tm2AzdsG2do/lGtb\nc6pl5tbKzJ1VZm61wuxqidnVMrOrZWZVSsyqlqmVS9QqJWZVSlTLolYpUS1nt5KEBCXB7GqZOdUy\nc2plKqVs3Uq5RLUkyiVRKYuSRKVUGl4ul0SllJWXSyPL1UqJWtqHO0iY9b68ySbPtNA2SebPrnLE\ngX0c0aVb9GC9wdYdQ2zdMciW7UNs7R/kif46T/QP8cTAUHbfX2fbwBDbBupsH6izbaDOjqHs8aZt\nA/QPNRgYarBjsM5APRisZ8uD9QZDjcn78SGREhs0AhqNIICyRKmU3Ws46Y0krXIqg6x/fiklsnJK\nbs31a5USsytlapUSQ43sGAfSvEIlaWSdapk51RKzKmWq5RK1SvZcQKppBo0GNCKGXzOrUqJSLhER\nNCKGa6TNfc9OyblWLjFYbwy/50ONbP16y/tcEsyqZOvPqmQ/AJrHGwH1RmN4/XK5lJI4iJE3IYtj\n5D1pxtH8UTGrUqKU3juA/qE6/ekzl0bev2pZVMslKqXscxmt+Z4EQfq3U218VjXb16yWHy4B9A9m\n+2tEtPygIXs/GiOxN7V+1qXSyOdVj6BeD4YaDbYNZH/32wbrlDQSezW9R9VyCYnhz6daFrMqZWZV\nS8M/oKrlYpuqI4KhRqTjmNk/rpxseky1XGLRvBqLCupQUG9kyQcY/lLsH2qwfaDO9sE6A0PZF99Q\no8FQPXt+MH1hDDUifTEy8nxkz9XTtrKkFumLN/vybzRi+ItQgnpjZN/NL7bWbbR+UTeTVD2CoXqk\ndaEe2XH0DzbYNjBEpVRibq1CX6WEYHhbg/UGm7cP8vDmOgP1kYQ0UuPPYmrG1ogs9oGhBoONoJS+\n1EX2xdt8D/uHdp0srySGa38lZRf7NpNE/1B9ly9cK14zoQUjCa+Z4ISyRJXWbU3irTX3oUYwVG9Q\nj5GkUpKyH3RD9eFkXEo/rrIfK9kPi/Lwj6QsaQ+m/zeD9ez/UvP/UGtCz1oosoTaiOzvrfn/pZlY\nJQ3/WKgP/9+M4b/r5jE1t/uZM4/iJQcXO6C/k43tJPvjL+9U5n7uu6+REk7/UJ1aajqsdPkVHZF9\nwWwfqDOUajL1VJNqfrkBw4m++cUY6culXBqp7UWqcdTrwUC9Plyryr6QsnVmpRpdpZwlvHqD4Zrt\nUD3VADslv1Sran7xNg01shpc/2BWY2regOHmW9BOP1SaTa6t24lo/hhoDP/waCblskaabudUy+w1\nq8LsWnn4/RusZ9seSPtuxilgsBHsGKzTP1hnx2BWs98xVKfeICV/0nvMcI211HwxDNfkRv/wqZSy\nZuVyScNljYjh97hWLg3/GBpM723zfao3GtTTD6ZmDb25vWp555q8JAbrDXYMNugfrGc1ptLIj6Fy\nKUt0zR8+rX8brU3jzcMZbh2Q2H/BnO5/0BPAycasAKWSmFPLmsbykEStIvdAtGnLf9lmZlY4Jxsz\nMyuck42ZmRXOycbMzArnZGNmZoVzsjEzs8I52ZiZWeGcbMzMrHAeiDORtAH49R6+fF/gkQkMp1fM\nxOOeiccMM/O4Z+Ixw+4f99ORoxWEAAAHdElEQVQjYsyxbpxsJoCk1XlGPZ1uZuJxz8Rjhpl53DPx\nmKG443YzmpmZFc7JxszMCudkMzE+M9UBTJGZeNwz8ZhhZh73TDxmKOi4fc7GzMwK55qNmZkVzslm\nnCSdKOluSWslnTvV8RRB0oGSrpN0l6Q7Jb09lS+StFLSPel+4VTHOtEklSWtkfSfafkZkq5Px/wl\nScVMmTqFJPVJ+rKkn6fP/Hen+2ct6S/T3/Ydkr4oafZ0/KwlXSppvaQ7WsrafrbKXJC+226TdOR4\n9u1kMw6SysCFwEnAocAZkg6d2qgKMQS8KyKeCxwDnJOO81xgVUQcDKxKy9PN24G7WpY/AnwyHfNj\nwFlTElWx/hn4VkQ8Bzic7Pin7Wct6QDgL4DlEfE8oAyczvT8rC8DThxV1umzPQk4ON3OBi4az46d\nbMbnaGBtRNwbEQPAlcApUxzThIuIByPi5vR4K9mXzwFkx3p5Wu1y4NSpibAYkpYAfwhcnJYFHAd8\nOa0yHY95PvBS4BKAiBiIiE1M88+abNbiOZIqwFzgQabhZx0RPwA2jiru9NmeAlwRmZ8CfZL239N9\nO9mMzwHAAy3L61LZtCVpGfAC4Hpgv4h4ELKEBDxl6iIrxKeA9wKNtLwPsCkihtLydPy8nwlsAD6X\nmg8vljSPafxZR8RvgI8B95Mlmc3ATUz/z7qp02c7od9vTjbjozZl07Z7n6S9gK8A74iILVMdT5Ek\nvRJYHxE3tRa3WXW6fd4V4Ejgooh4AfAE06jJrJ10juIU4BnA04B5ZE1Io023z3osE/r37mQzPuuA\nA1uWlwC/naJYCiWpSpZoPh8RX03FDzer1el+/VTFV4AXA/9D0n1kzaPHkdV0+lJTC0zPz3sdsC4i\nrk/LXyZLPtP5s/4D4FcRsSEiBoGvAr/H9P+smzp9thP6/eZkMz43AgenXis1spOKV09xTBMunau4\nBLgrIj7R8tTVwIr0eAXw9cmOrSgRcV5ELImIZWSf63cj4nXAdcBr0mrT6pgBIuIh4AFJz05FrwB+\nxjT+rMmaz46RNDf9rTePeVp/1i06fbZXA2emXmnHAJubzW17whd1jpOkk8l+8ZaBSyPig1Mc0oST\ndCzwQ+B2Rs5fvI/svM1VwFKy/7CnRcTok489T9LLgXdHxCslPZOsprMIWAO8PiL6pzK+iSbpCLJO\nETXgXuCNZD9Mp+1nLenvgNeS9bxcA/wp2fmJafVZS/oi8HKykZ0fBs4H/i9tPtuUeP83We+1bcAb\nI2L1Hu/bycbMzIrmZjQzMyuck42ZmRXOycbMzArnZGNmZoVzsjEzs8I52diMIykkfbxl+d2S/naC\ntn2ZpNeMvea493NaGpH5ulHlyyRtl3RLy+3MCdzvy5sjYJvtjsrYq5hNO/3AqyX9Y0Q8MtXBNEkq\nR0Q95+pnAX8eEde1ee6XEXHEBIZmNm6u2dhMNEQ29e1fjn5idM1E0uPp/uWSvi/pKkm/kPRhSa+T\ndIOk2yU9q2UzfyDph2m9V6bXlyV9VNKNaW6QN7Vs9zpJXyC7aHZ0PGek7d8h6SOp7H8BxwL/Jumj\neQ9a0uOSPi7pZkmrJC1O5UdI+mmK62st85kcJOk7km5Nr2ke414ame/m8+niP9J78rO0nY/ljctm\nBicbm6kuBF4nacFuvOZwsvltfgd4A3BIRBxNdrX921rWWwa8jGx6gn+TNJusJrI5Il4IvBD4M0nP\nSOsfDfx1ROw0F5Kkp5HNqXIccATwQkmnRsQHgNXA6yLiPW3ifNaoZrSXpPJ5wM0RcSTwfbKrxwGu\nAP4qIp5PlvCa5Z8HLoyIw8nGCmsOVfIC4B1kczg9E3ixpEXAq4DD0nb+Yaw302YWJxubkdKo1VeQ\nTZqV141pbp9+4JfAt1P57WQJpumqiGhExD1kw708BziebJypW8iG+dmHbFIqgBsi4ldt9vdC4Htp\ngMghsi//l+aI85cRcUTL7YepvAF8KT3+D+DYlGz7IuL7qfxy4KWS9gYOiIivAUTEjojY1hLvuoho\nALekY98C7AAulvRqsuFNzIY52dhM9imyGse8lrIh0v+L1DzUOhVw67hYjZblBjuf/xw9BlSQDdf+\ntpYE8IyIaCarJzrE126I94nUbayqbvtufR/qQCUlw6PJRgY/FfjW+MOz6cTJxmasNJDkVew83e99\nwFHp8SlAdQ82fZqkUjrH8UzgbuBa4C1pqgYkHaJsUrJurgdeJmlfZVOQn0HW/LWnSoyMYvzHwI8i\nYjPwWEtT2xuA76ea3zpJp6Z4Z0ma22nDyuY6WhAR15A1sbmDgu3EvdFspvs48NaW5c8CX5d0A9l8\n7J1qHd3cTZYU9gPeHBE7JF1M1tx0c6oxbWCMaYYj4kFJ55ENdS/gmojIM8z9s1JzXdOlEXEB2bEc\nJukmstkoX5ueX0F2bmkuI6M8Q5Z4Pi3pA8AgcFqXfe5N9r7NTrHu0vnCZjaP+mw2Q0h6PCL2muo4\nbGZyM5qZmRXONRszMyucazZmZlY4JxszMyuck42ZmRXOycbMzArnZGNmZoVzsjEzs8L9fwDC3OVB\nV7xSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11388b278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Total cost of all examples in an epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wax': array([[  1.62434536e-02,   4.91484254e-01,  -1.82446832e+00, ...,\n",
      "          8.66793980e-02,  -1.93698655e+00,   6.16819446e-02],\n",
      "       [ -9.35769434e-03,  -2.97614798e+00,  -1.82043177e+00, ...,\n",
      "         -6.96521344e-01,   1.07062823e+00,   9.72353259e-01],\n",
      "       [ -2.08894233e-03,  -5.29676308e-01,  -1.16259239e+00, ...,\n",
      "          5.69273581e-01,  -1.26397701e+00,   5.69825697e-01],\n",
      "       ..., \n",
      "       [  2.19477494e-03,  -4.19920638e+00,  -6.95892914e-01, ...,\n",
      "          5.01857073e-01,   1.51578058e+00,   6.19303547e-01],\n",
      "       [  6.03035910e-03,  -9.04365779e-02,   2.42451456e+00, ...,\n",
      "         -9.55558294e-01,  -3.07039300e+00,  -9.87766723e-01],\n",
      "       [  8.73005837e-03,   4.84590273e-01,   1.84956588e+00, ...,\n",
      "         -2.87467500e-01,  -3.26461822e+00,  -2.89448892e-01]]), 'Waa': array([[-0.20705129, -0.29739349, -1.06949436, ..., -0.80124283,\n",
      "        -0.52911181,  0.12166674],\n",
      "       [-0.14933432,  0.15788951,  0.28942445, ...,  0.57998869,\n",
      "         0.26887917,  0.17965901],\n",
      "       [ 0.3533233 , -0.04614907, -0.52813945, ...,  0.31994226,\n",
      "         0.03056483, -0.46247832],\n",
      "       ..., \n",
      "       [ 0.27750146, -0.10270731, -0.11798702, ..., -0.28665119,\n",
      "        -0.12522321, -0.06507644],\n",
      "       [ 0.46683877, -0.04218577, -0.60811399, ...,  0.30537013,\n",
      "         0.30146781,  0.14823904],\n",
      "       [ 0.03043832,  0.67175394,  0.183136  , ..., -0.57909942,\n",
      "         0.18076647, -1.01795602]]), 'Wya': array([[-0.10165544, -0.57319382,  0.0298017 , ..., -0.24061302,\n",
      "        -0.70045595,  0.16778264],\n",
      "       [-0.03176564,  0.29951273, -0.15050344, ...,  0.95262005,\n",
      "        -0.21924815, -0.24157412],\n",
      "       [-0.23649075,  0.00502638, -0.07815358, ...,  0.09524419,\n",
      "         0.329523  , -0.39888545],\n",
      "       ..., \n",
      "       [-1.25179195,  0.17180863,  0.55527216, ..., -0.73800758,\n",
      "         0.37757562, -0.51051615],\n",
      "       [ 0.33702359, -0.44534844,  0.27510604, ...,  0.36719548,\n",
      "        -0.0673952 , -0.06678156],\n",
      "       [ 0.24384648, -0.10055128,  0.08843364, ...,  0.11990394,\n",
      "        -0.78638909,  0.13195864]]), 'b': array([[-0.12337217],\n",
      "       [ 0.75065323],\n",
      "       [-1.25718043],\n",
      "       [-0.13203091],\n",
      "       [-3.71304008],\n",
      "       [-0.48379096],\n",
      "       [ 0.91449338],\n",
      "       [-0.28658613],\n",
      "       [ 1.40341795],\n",
      "       [ 1.27590309],\n",
      "       [ 1.27732962],\n",
      "       [ 0.74008723],\n",
      "       [-0.38842923],\n",
      "       [ 0.56371067],\n",
      "       [-0.05037975],\n",
      "       [-1.95528117],\n",
      "       [-0.82119989],\n",
      "       [-0.46768939],\n",
      "       [-2.88768566],\n",
      "       [ 2.01830585],\n",
      "       [ 0.888861  ],\n",
      "       [-0.87087773],\n",
      "       [-1.63266686],\n",
      "       [-0.5269445 ],\n",
      "       [ 1.50393558],\n",
      "       [-0.81942986],\n",
      "       [-2.52188194],\n",
      "       [ 0.07467177],\n",
      "       [-0.50705354],\n",
      "       [-1.73894992],\n",
      "       [ 1.7956358 ],\n",
      "       [-0.41884843],\n",
      "       [ 3.63813673],\n",
      "       [ 2.72705589],\n",
      "       [-1.45991046],\n",
      "       [ 0.88370468],\n",
      "       [-0.20097498],\n",
      "       [ 1.72541198],\n",
      "       [ 0.40887501],\n",
      "       [-0.41603243],\n",
      "       [ 1.52785227],\n",
      "       [-0.31836004],\n",
      "       [ 1.00726698],\n",
      "       [ 1.85045989],\n",
      "       [-1.90239501],\n",
      "       [-3.77062876],\n",
      "       [-2.10137312],\n",
      "       [ 3.70561994],\n",
      "       [ 1.18492828],\n",
      "       [-0.71679228]]), 'by': array([[ 0.91626114],\n",
      "       [ 1.77685806],\n",
      "       [-0.91148869],\n",
      "       [ 0.74002925],\n",
      "       [ 0.90328505],\n",
      "       [ 0.8248188 ],\n",
      "       [-1.89471056],\n",
      "       [-0.19428579],\n",
      "       [-0.29208188],\n",
      "       [ 0.19889041],\n",
      "       [-0.9209903 ],\n",
      "       [ 0.17563706],\n",
      "       [ 2.8362516 ],\n",
      "       [ 1.3870962 ],\n",
      "       [ 2.2907842 ],\n",
      "       [-0.76496965],\n",
      "       [-0.56043005],\n",
      "       [-2.02443753],\n",
      "       [ 0.78312747],\n",
      "       [ 0.88602436],\n",
      "       [-0.51915097],\n",
      "       [-1.61764961],\n",
      "       [-0.19695408],\n",
      "       [-1.65043868],\n",
      "       [-2.17205674],\n",
      "       [ 0.19060585],\n",
      "       [-0.19002492]])}\n"
     ]
    }
   ],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_new(parameters, character_to_indexes, nick):\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    vocab_size = by.shape[0]   # Size of Output vector y which is same at all time steps.\n",
    "    \n",
    "    n_a = Waa.shape[0]         # Size of activation vector.\n",
    "    \n",
    "    # Initializing vector x for first character input. It will be zero.\n",
    "    # We could create a vector of random values as well.\n",
    "    if (len(nick) == 0):\n",
    "        x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Initializing a_prev as zeros.\n",
    "        a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Initializing an empty list of indices, which will contain list of indices from which we generate names.\n",
    "    \n",
    "        indices = []\n",
    "    \n",
    "    # Flag to detect a newline character.\n",
    "        next_character = -1\n",
    "    \n",
    "    # Looping over time steps 't' to forward propagate and predict output y i.e. index of character generated.\n",
    "    # We sample from a probability distribution randomly at each time step.\n",
    "    # Also, we don't want to have names larger than say(20) characters. So, we use counter to stop the loop.\n",
    "    # Counter will also prevent from entering into an infinite loop.\n",
    "    \n",
    "        counter = 0\n",
    "    \n",
    "        newline_character = character_to_indexes['\\n']\n",
    "    \n",
    "    # Looping over the time steps 't' \n",
    "        while( next_character != newline_character and counter != 20):\n",
    "       \n",
    "        # Forward Propagating 'x'\n",
    "        # Calculating the activation.\n",
    "            a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        # Calculating the output vector.\n",
    "            z = np.dot(Wya,a) + by\n",
    "        # Taking softmax of the output vector.\n",
    "            y = softmax(z)\n",
    "        \n",
    "        \n",
    "        # np.random.choice chooses an index from [0,1,...,26] using probability distribution given by y.\n",
    "        # y is vector having probability of choosing an index.\n",
    "        # We could also choose argmax of y but we want to sample randomly.\n",
    "            next_character = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "        \n",
    "        # Appending the index to the indices list.\n",
    "            indices.append(next_character)\n",
    "        \n",
    "        # Overwriting the input character as the one corresponding to the sampled index.\n",
    "        # To be input for next timer step 't'.\n",
    "            x = np.zeros((vocab_size,1))\n",
    "            x[next_character] = 1\n",
    "        \n",
    "        # Updating a_prev to be 'a' for next time step.\n",
    "        \n",
    "            a_prev = a\n",
    "        \n",
    "        # Increasing the counter.\n",
    "            counter += 1\n",
    "        \n",
    "    \n",
    "        if (counter == 20):\n",
    "            indices.append(character_to_indexes['\\n'])\n",
    "    \n",
    "    else:\n",
    "        x = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Initializing a_prev as zeros.\n",
    "        a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Initializing an empty list of indices, which will contain list of indices from which we generate names.\n",
    "    \n",
    "        indices = []\n",
    "    \n",
    "    # Flag to detect a newline character.\n",
    "        next_character = -1\n",
    "    \n",
    "    # Looping over time steps 't' to forward propagate and predict output y i.e. index of character generated.\n",
    "    # We sample from a probability distribution randomly at each time step.\n",
    "    # Also, we don't want to have names larger than say(20) characters. So, we use counter to stop the loop.\n",
    "    # Counter will also prevent from entering into an infinite loop.\n",
    "    \n",
    "        counter = 0\n",
    "    \n",
    "        newline_character = character_to_indexes['\\n']\n",
    "        \n",
    "        for i in range(len(nick)):\n",
    "            # Forward Propagating 'x'\n",
    "        # Calculating the activation.\n",
    "            a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        # Calculating the output vector.\n",
    "            z = np.dot(Wya,a) + by\n",
    "        # Taking softmax of the output vector.\n",
    "            y = softmax(z)\n",
    "            \n",
    "        # np.random.choice chooses an index from [0,1,...,26] using probability distribution given by y.\n",
    "        # y is vector having probability of choosing an index.\n",
    "        # We could also choose argmax of y but we want to sample randomly.\n",
    "            next_character = character_to_indexes[nick[i]]\n",
    "        \n",
    "        # Appending the index to the indices list.\n",
    "            indices.append(next_character)\n",
    "        \n",
    "        # Overwriting the input character as the one corresponding to the sampled index.\n",
    "        # To be input for next timer step 't'.\n",
    "            x = np.zeros((vocab_size,1))\n",
    "            x[next_character] = 1\n",
    "        \n",
    "        # Updating a_prev to be 'a' for next time step.\n",
    "        \n",
    "            a_prev = a\n",
    "        \n",
    "        # Increasing the counter.\n",
    "            counter += 1\n",
    "        \n",
    "    \n",
    "        while(next_character != newline_character and counter != 20):\n",
    "       \n",
    "        # Forward Propagating 'x'\n",
    "        # Calculating the activation.\n",
    "            a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        # Calculating the output vector.\n",
    "            z = np.dot(Wya,a) + by\n",
    "        # Taking softmax of the output vector.\n",
    "            y = softmax(z)\n",
    "        \n",
    "        \n",
    "        # np.random.choice chooses an index from [0,1,...,26] using probability distribution given by y.\n",
    "        # y is vector having probability of choosing an index.\n",
    "        # We could also choose argmax of y but we want to sample randomly.\n",
    "            next_character = np.random.choice(list(range(vocab_size)), p = y.ravel())\n",
    "        \n",
    "        # Appending the index to the indices list.\n",
    "            indices.append(next_character)\n",
    "        \n",
    "        # Overwriting the input character as the one corresponding to the sampled index.\n",
    "        # To be input for next timer step 't'.\n",
    "            x = np.zeros((vocab_size,1))\n",
    "            x[next_character] = 1\n",
    "        \n",
    "        # Updating a_prev to be 'a' for next time step.\n",
    "        \n",
    "            a_prev = a\n",
    "        \n",
    "        # Increasing the counter.\n",
    "            counter += 1\n",
    "        \n",
    "    \n",
    "        if (counter == 50):\n",
    "            indices.append(character_to_indexes['\\n'])\n",
    "        \n",
    "            \n",
    "    return indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter starting/nick name: claire\n"
     ]
    }
   ],
   "source": [
    "nick = input(\"Enter starting/nick name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clairey\n",
      "Claire\n",
      "Claire\n",
      "Clairea\n",
      "Clairer\n",
      "Claire\n",
      "Claire\n",
      "Clairen\n",
      "Claire\n",
      "Clairen\n",
      "Clairencely\n",
      "Claire\n",
      "Claire\n",
      "Clairee\n",
      "Clairen\n",
      "Clairen\n",
      "Claireth\n",
      "Clairene\n",
      "Claire\n",
      "Clairevi\n"
     ]
    }
   ],
   "source": [
    "#generates_names = set()\n",
    "for i in range(20):\n",
    "    check = sample_new(parameters ,character_to_indexes, nick)\n",
    "    name = print_sample(check, index_to_characters)\n",
    "    #generated_names.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training Using LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'isabella'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"babygirls.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]      # examples is list of all training examples.\n",
    "examples[4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing important libraries:\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector,SimpleRNN\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras uses one-hot encoding for sequences :\n",
    "# Converting names to indices :\n",
    "with open(\"babygirls.txt\") as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(27, return_sequences=True , input_shape=(None,27), activation = 'tanh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, None, 27)          5940      \n",
      "=================================================================\n",
      "Total params: 5,940\n",
      "Trainable params: 5,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 27)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.summary())\n",
    "model.input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0126594 ,  0.20984386, -0.10330032, ..., -0.10230265,\n",
       "          0.04305981, -0.17863609],\n",
       "        [ 0.1659434 ,  0.07140179, -0.12459808, ..., -0.02105293,\n",
       "         -0.11039771, -0.0960347 ],\n",
       "        [ 0.03899634,  0.09243278, -0.0483529 , ..., -0.03800078,\n",
       "          0.17989351,  0.02427579],\n",
       "        ..., \n",
       "        [-0.13841745, -0.03599533,  0.09876876, ..., -0.19090757,\n",
       "          0.06479667,  0.03887002],\n",
       "        [ 0.1752079 , -0.15396276, -0.15619208, ..., -0.05465056,\n",
       "          0.14368974, -0.16434957],\n",
       "        [ 0.06567059, -0.08544584, -0.17480831, ..., -0.20616415,\n",
       "          0.1740175 , -0.19782576]], dtype=float32),\n",
       " array([[-0.08276286,  0.09480623,  0.00747002, ..., -0.0846485 ,\n",
       "          0.06224215,  0.1561757 ],\n",
       "        [ 0.01877646,  0.02737487, -0.14958543, ...,  0.17500615,\n",
       "         -0.03214522, -0.0259013 ],\n",
       "        [ 0.01590555,  0.19464493,  0.06376348, ...,  0.03016294,\n",
       "         -0.15988843, -0.06097747],\n",
       "        ..., \n",
       "        [ 0.01887506, -0.01427871,  0.0228591 , ..., -0.03933524,\n",
       "         -0.00674854, -0.07811131],\n",
       "        [-0.01295147, -0.0650268 ,  0.01227079, ..., -0.10863958,\n",
       "         -0.18780921, -0.09450911],\n",
       "        [-0.08484084,  0.05482667, -0.00600495, ..., -0.10617802,\n",
       "         -0.00094825, -0.13729601]], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.], dtype=float32)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 16.1181 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2230 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.8317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.3603 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0934 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.4893 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3602 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6631 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.9793 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.7437 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.3583 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3595 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.5900 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5326 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6561 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5842 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.5500 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6292 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.6640 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6890 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.8049 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.1825 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3811 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7936 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.3073 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2991 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.3119 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7973 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.9004 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5285 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5766 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.5028 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9924 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.9110 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 13.2482 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6862 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3552 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.4459 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4046 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8597 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2662 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8209 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5716 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 14.2062 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.8006 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6863 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.3417 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0785 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.9033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.4667 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1021 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6722 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.5305 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5263 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.1638 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.6258 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2614 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2803 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.1080 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8205 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.9797 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.2311 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.7718 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.9749 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1973 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0703 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5711 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.8901 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.7144 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.4217 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0754 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4295 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2873 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4981 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2352 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.1463 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1266 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.8190 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.2418 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8312 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8235 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7645 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7760 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.8053 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1636 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4146 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0951 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.3542 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.9545 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2732 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.3214 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.5323 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.9933 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.7585 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.9369 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6052 - acc: 0.2857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.3830 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.0368 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8882 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4377 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9515 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3216 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4772 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.1484 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.1528 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.3042 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1389 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7292 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0074 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.0675 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.7471 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4420 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7852 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.6840 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3144 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0303 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3095 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0873 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.3514 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.6931 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.6263 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.3812 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2599 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6176 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1309 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.2835 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.9626 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 12.1019 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1567 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2420 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.4882 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1854 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.6615 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.4024 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1514 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.3467 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.9122 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9175 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8640 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2518 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8088 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.5560 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.2984 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2326 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5475 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4586 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5080 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1845 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6503 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0927 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8282 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9670 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9391 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.1525 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.1022 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.4337 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4573 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.6229 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1251 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2916 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.5608 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2437 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.1279 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.5329 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2899 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2522 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5316 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.8650 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5190 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6519 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2269 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4191 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.4379 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5230 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5225 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.2682 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9163 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4842 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2791 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.3282 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1453 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1033 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6698 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.6494 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.6468 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.9008 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.4805 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7329 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6813 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9992 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8464 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9567 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2540 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3022 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6431 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2502 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5465 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0487 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9765 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.5466 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.1129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.4537 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4625 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.8493 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2847 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3026 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.0827 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.0001 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9795 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.9988 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.5129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.5637 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0997 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.1161 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2657 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4180 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.9941 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.4371 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2442 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.7529 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.2752 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.7917 - acc: 0.0909\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.9104 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.8744 - acc: 0.0909\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.2020 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.1139 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1029 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4552 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2236 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.0033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.9607 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1736 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0483 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7318 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.8745 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.2723 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.1434 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.9148 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1392 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1293 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1343 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.7520 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1389 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0446 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2700 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2909 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6801 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8102 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0649 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.7155 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.9991 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7195 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7845 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9830 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4997 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.6946 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.4597 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4987 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.4955 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.4408 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2385 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.8549 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0268 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6037 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9580 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0871 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.2566 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.1826 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1871 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.5835 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.3331 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5074 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 12.5840 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.6495 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3569 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3026 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6425 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6545 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.9694 - acc: 0.3000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.6107 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8318 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.0743 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6633 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0645 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.3257 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3026 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5852 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.6248 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.5889 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3654 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6385 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3552 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1033 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1921e-07 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1989 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1932 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5207 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6919 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1931 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.4745 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.1918 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.6405 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.8464 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5249 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.3608 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4942 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1954 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.1754 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3810 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.4206 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.9078 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4344 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.9545 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2696 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2890 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.5022 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1374 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2642 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2455 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.6925 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.0886 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0461 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0801 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.8690 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.1033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8989 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6739 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.1413 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2228 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0148 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.8209 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.5906 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.2373 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 13.0602 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.5615 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0465 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6063 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4924 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0148 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.2580 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.0590 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.1906 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.7169 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1223 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2609 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.1246 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2969 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1410 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.4822 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.5486 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1310 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.6918 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.5045 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.4602 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3756 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0796 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.6945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3426 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1120 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.3692 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 12.0886 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7429 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2502 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5120 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8838 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.9545 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.2903 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2127 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.2756 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3012 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.0436 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.9174 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2336 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1346 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6863 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7582 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.5192 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6074 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3849 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9078 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.5464 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.4956 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.8820 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0863 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.4633 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.6056 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3556 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3157 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.9749 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3768 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8455 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.7001 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.0012 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6787 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9277 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2240 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4520 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9827 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4187 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.0266 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.1446 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0331 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.4497 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.3458 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4433 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2337 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.2181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2298 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.1381 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.1656 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2947 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.0883 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0821 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.1361 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2015 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0738 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8518 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1301 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4220 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.6740 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.8170 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.1420 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9545 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5371 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6724 - acc: 0.2857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2621 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7645 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9603 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.9246 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5372 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7500 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.3645 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9556 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.5556 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0443 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9603 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0471 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9539 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7502 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2111 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.2982 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.2852 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0794 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.0661 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.7282 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.8461 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4371 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8770 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.4017 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8013 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0830 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7657 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8247 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6810 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5289 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1574 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.9058 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5875 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6829 - acc: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.6057 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3425 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9687 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1533 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0839 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2576 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.4049 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.9347 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7787 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.0695 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9823 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.9502 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.2363 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.2236 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.3157 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3026 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.5804 - acc: 0.0909\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3439 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3896 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.3340 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1586 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.5334 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.7508 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5278 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.6418 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.5734 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1017 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6116 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0470 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.8432 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9930 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2834 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5131 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.0989 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.3919 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6934 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5967 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.6850 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7941 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.1228 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0776 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3451 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3085 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1205 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1877 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4563 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9545 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7688 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7191 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6273 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.2091 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9643 - acc: 0.2222\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6052 - acc: 0.2857\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.1230 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.0407 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2477 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.8945 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7887 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.1216 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2880 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1266 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1109 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.6960 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1439 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0736 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1513 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3011 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.4317 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1419 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.1341 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.7452 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5251 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.5542 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.6581 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.1339 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.9545 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1495 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5382 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.1947 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2505 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.7670 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1137 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.9970 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.0884 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4997 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.8349 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1057 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6633 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1341 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.6855 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5463 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2813 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.0972 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.5204 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4401 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.2244 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7491 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.3255 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.3985 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.2072 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.8060 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1168 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.4120 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3084 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3026 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.3753 - acc: 0.1429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.6188 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.8556 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9471 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.3504 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.6736 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3337 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.5250 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0825 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.1258 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7906 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2495 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.4033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3287 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2273 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.9331 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5876 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.4741 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2594 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.6052 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9866 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.0852 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0569 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2371 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3994 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.5001 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3065 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4610 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7167 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.6626 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3335 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.7096 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6654 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6935 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.2459 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3734 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.2908 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.0467 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.1855 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9864 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.7107 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8019 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 7.2189 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.0083 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3963 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3496 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.7779 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8108 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.4907 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5511 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0369 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6452 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2982 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.3595 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.6474 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.6481 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.7574 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.0599 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.8023 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.3750 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.4020 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.3727 - acc: 0.3333\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2509 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0693 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.1497 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7166 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0295 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4077 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.2845 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.4317 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.6354 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.4296 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.0705 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1636 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5303 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2835 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.9240 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.3706 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2236 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.7980 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8155 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6793 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.9393 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.5129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.1091 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.3784 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.0948 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5916 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.4384 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5786 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4528 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9406 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.2545 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0918 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.4493 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5216 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5877 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9078 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1948 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0736 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.5129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2637 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.0886 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.3333\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.9370 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.9062 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.4871 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2147 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1194 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.3591 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7600 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0757 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.2268 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0711 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.5314 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.0743 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.2155 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.1960 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.8347 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.0426 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6722 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.1802 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.8510 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0481 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.7223 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.4042 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.3250 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 16.1181 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.3117 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4713 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2281 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0705 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0763 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 12.8945 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.3951 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.8662 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.2758 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.7497 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8155 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.0738 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.4712 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6210 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.8856 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.7454 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.3727 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.3469 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 12.3358 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.8374 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3343 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6030 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6262 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.7080 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2413 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.1165 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.2296 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.7404 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7487 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.7144 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.8143 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1585 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8999 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.5767 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.1636 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3026 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.5816 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1717 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.1377 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6740 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.7454 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.2995 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5002 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.9590 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1406 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.5326 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1989 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.7986 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.5393 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.4808 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1257 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.0300 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.2827 - acc: 0.1000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.6709 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0653 - acc: 0.2222\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.6439 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5105 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.5129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5345 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0304 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.2310 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.1012 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.1372 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.5421 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.3614 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.1446 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0590 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8013 - acc: 0.1111\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.6052 - acc: 0.4286\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.8250 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3449 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.5129 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.6765 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.1252 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9510 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.8092 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0009 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1358 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1949 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.1459 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.0443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0295 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.0148 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.6228 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6709 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0255 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6830 - acc: 0.2000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.2283 - acc: 0.1429\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.0590 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.0738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.4506 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.7454 - acc: 0.1667\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.9545 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0929 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1632 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4466 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5335 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.7454 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for k in range(1):\n",
    "    for j in range(len(examples)):\n",
    "         \n",
    "            X = [0] + [character_to_indexes[ch] for ch in examples[j]] \n",
    "            Y = X[1:] + [character_to_indexes[\"\\n\"]]\n",
    "        \n",
    "            X_train = np.zeros((len(X),27))\n",
    "            Y_train = np.zeros((len(Y),27))\n",
    "        \n",
    "            for i in range(len(X)):\n",
    "                X_train[i][X[i]] = 1\n",
    "                Y_train[i][Y[i]] = 1\n",
    "        \n",
    "        \n",
    "            X_train = X_train.reshape(1,len(X),27)\n",
    "            Y_train = Y_train.reshape(1,len(Y),27)\n",
    "            history = model.fit(X_train,Y_train)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
